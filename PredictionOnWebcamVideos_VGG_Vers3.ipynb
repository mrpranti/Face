{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrpranti/Face/blob/main/PredictionOnWebcamVideos_VGG_Vers3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51Bmtg7Y6Wey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa7deea-4066-4568-d3b7-c8bb081cb2a5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/MyDrive/DL Group Project\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1q1oh4KNPvyl3ACTsbffzNd5ALcMrB42e/DL Group Project\n",
            "\u001b[0m\u001b[01;34m'Draft Version'\u001b[0m/      \u001b[01;34mProject_Code\u001b[0m/           \u001b[01;34m'Project Paper'\u001b[0m/\n",
            " \u001b[01;34mLecture_Notebooks\u001b[0m/  \u001b[01;34m'Project Organization'\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNKgimPD6ZWN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.style.use('ggplot')\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa71aS8w6bS0"
      },
      "source": [
        "test_csv_path = 'Project_Code/Data/test.csv'\n",
        "keypoint_model_path = 'Project_Code/OutputFolder/Model03_400epochs'\n",
        "face_model_path = 'Project_Code/FaceRecognizerModel'\n",
        "output_path = 'Project_Code/WebcamTest/'\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Size of images\n",
        "SIZE = 96"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9xTSnECu2-M"
      },
      "source": [
        "# load model\n",
        "modelFile = face_model_path + \"/res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "configFile = face_model_path + \"/deploy.prototxt.txt\"\n",
        "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdp9qgOa6g24"
      },
      "source": [
        "# 1) Prepare Facial Keypoints Detection Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HLiUOVXT-dC"
      },
      "source": [
        "# load additional libraries \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6XIEZ6W6mtj"
      },
      "source": [
        "# to understand the idea behind the following structure: \n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "class FaceKeypointModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FaceKeypointModel, self).__init__()\n",
        "        \n",
        "        # modle adapted from the concept of VGG net and facial-keypoint-detection.ipynb\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 3, padding = 2)\n",
        "        self.batch_normal1 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding =2 ) \n",
        "        self.batch_normal2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv3= nn.Conv2d(32, 64, kernel_size = 3, padding = 2)\n",
        "        self.batch_normal3 = nn.BatchNorm2d(64)\n",
        "         \n",
        "        self.conv4= nn.Conv2d(64, 64, kernel_size = 3, padding = 2)\n",
        "        self.batch_normal4 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv5= nn.Conv2d(64, 96, kernel_size = 3, padding = 2)\n",
        "        self.batch_normal5 = nn.BatchNorm2d(96)\n",
        "\n",
        "        self.conv6= nn.Conv2d(96, 96, kernel_size = 3, padding = 2)\n",
        "        self.batch_normal6 = nn.BatchNorm2d(96)\n",
        "\n",
        "        self.conv7= nn.Conv2d(96, 128, kernel_size = 3, padding = 2)\n",
        "        self.batch_normal7 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv8= nn.Conv2d(128, 128, kernel_size = 3, padding = 2)\n",
        "        self.batch_normal8 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv9= nn.Conv2d(128, 256, kernel_size = 3, padding = 2)\n",
        "        self.batch_normal9 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv10= nn.Conv2d(256, 256, kernel_size = 3, padding = 2)\n",
        "        self.batch_normal10 = nn.BatchNorm2d(256) \n",
        "\n",
        "        self.conv11= nn.Conv2d(256, 512, kernel_size = 3, padding = 2)\n",
        "        self.batch_normal11 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv12= nn.Conv2d(512, 512, kernel_size = 3, padding = 2)\n",
        "        self.batch_normal12 = nn.BatchNorm2d(512)  \n",
        "\n",
        "        self.fc1 = nn.Linear(512, 512)\n",
        "        self.fc2 = nn.Linear(512, 30 )\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.activ = nn.LeakyReLU(0.1)\n",
        "        self.dropout = nn.Dropout2d(p=0.2)\n",
        "    def forward(self, x):\n",
        "         x = self.conv1(x)\n",
        "         x = self.activ(x)\n",
        "         x = self.batch_normal1(x)\n",
        "\n",
        "         x = self.conv2(x)\n",
        "         x = self.activ(x)\n",
        "         x = self.batch_normal2(x)\n",
        "         x = self.pool(x)\n",
        "\n",
        "         x = self.conv3(x)\n",
        "         x = self.activ(x)\n",
        "         x = self.batch_normal3(x)\n",
        "\n",
        "         x = self.conv4(x)\n",
        "         x = self.activ(x)\n",
        "         x = self.batch_normal4(x)\n",
        "         x = self.pool(x)\n",
        "\n",
        "         x = self.conv5(x)\n",
        "         x = self.activ(x)\n",
        "         x = self.batch_normal5(x)\n",
        "\n",
        "         x = self.conv6(x)\n",
        "         x = self.activ(x)\n",
        "         x = self.batch_normal6(x)\n",
        "         x = self.pool(x)\n",
        "\n",
        "         x = self.conv7(x)\n",
        "         x = self.activ(x)\n",
        "         x = self.batch_normal7(x)\n",
        "\n",
        "         x = self.conv8(x)\n",
        "         x = self.activ(x)\n",
        "         x = self.batch_normal8(x)\n",
        "         x = self.pool(x)\n",
        "\n",
        "         x = self.conv9(x)\n",
        "         x = self.activ(x)\n",
        "         x = self.batch_normal9(x)\n",
        "\n",
        "         x = self.conv10(x)\n",
        "         x = self.activ(x)\n",
        "         x = self.batch_normal10(x)\n",
        "         x = self.pool(x)\n",
        "\n",
        "         x = self.conv11(x)\n",
        "         x = self.activ(x)\n",
        "         x = self.batch_normal11(x)\n",
        "\n",
        "         x = self.conv12(x)\n",
        "         x = self.activ(x)\n",
        "         x = self.batch_normal12(x)\n",
        "\n",
        "        \n",
        "      \n",
        "\n",
        "         # bs is the batch size or respectively the number of instances \n",
        "         # simultaneously loaded into the model\n",
        "         bs, _, _, _ = x.shape \n",
        "\n",
        "         # The next step finally transforms the images into 1-dim vectors of lenght 128\n",
        "         x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
        "         # Apply dropout for regularization and preventing the co-adaptation of neurons\n",
        "         # https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html?highlight=dropout#torch.nn.Dropout\n",
        "         x = self.dropout(x)\n",
        "         # apply a classic linear function\n",
        "         x = self.fc1(x)\n",
        "         out = self.fc2(x) \n",
        "         return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKDOySGQ8k4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f190c727-c666-48c5-ef87-70822d136693"
      },
      "source": [
        "model = FaceKeypointModel().to(DEVICE)\n",
        "# load the model checkpoint\n",
        "checkpoint = torch.load(f\"{keypoint_model_path}/model.pth\")\n",
        "# load model weights state_dict\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FaceKeypointModel(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (batch_normal1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (batch_normal2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (batch_normal3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (batch_normal4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv5): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (batch_normal5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv6): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (batch_normal6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv7): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (batch_normal7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (batch_normal8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (batch_normal9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (batch_normal10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (batch_normal11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (batch_normal12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=30, bias=True)\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (activ): LeakyReLU(negative_slope=0.1)\n",
              "  (dropout): Dropout2d(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rguK0BhiyGmV"
      },
      "source": [
        "# 2) Face Recognition on Webcam Image using the new model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqMjA8hdyFwp"
      },
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W1oMKLqySCN"
      },
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5F3oNqeyXWl"
      },
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  ###### start: new code #####\n",
        "  # get photo data\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # get OpenCV format image\n",
        "  img = js_to_image(data) \n",
        "\n",
        "  # trun color picture to grayscale image -> needed for keypoint prediction\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "  h, w = img.shape[:2]\n",
        "  blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0, (300, 300), (104.0, 117.0, 123.0))\n",
        "  # what is blob?\n",
        "  # shape: (1, 3, 300, 300) -> 3 color channels, size: 300 x 300\n",
        "  net.setInput(blob)\n",
        "  faces = net.forward()\n",
        "  # what is faces?\n",
        "  # faces.shape: (1, 1, 200, 7)\n",
        "  # Theoretically, 200 faces could be identified. \n",
        "  # For i-th face we have the following attributes: \n",
        "  #  - confidence that this is really a face: [0,0,i-1,2]\n",
        "  #  - coordinates for the face bounding box: [0,0,i-1,3:7] (x,y,x1,y2)\n",
        "\n",
        "  # add face bounding boxes to image\n",
        "  for i in range(faces.shape[2]):\n",
        "    confidence = faces[0, 0, i, 2]\n",
        "    if confidence > 0.5:\n",
        "      box = faces[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "      (x1, y1, x2, y2) = box.astype(\"int\")\n",
        "      cv2.rectangle(img, (x1,y1), (x2, y2), (255, 0, 0), 2) \n",
        "\n",
        "      # keypoint prediction\n",
        "      x_coord, y_coord = make_prediction(gray, x1, y1, x2, y2)\n",
        "      for i in range(len(x_coord)):\n",
        "          img = cv2.circle(img, (x_coord[i],y_coord[i]), radius=3, color=(0, 0, 255), thickness=-1)\n",
        "\n",
        "  cv2.imwrite(output_path + filename, img)\n",
        "\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzFsIS47A-6M"
      },
      "source": [
        "def make_prediction(gray_img, x1, y1, x2, y2):\n",
        "  # extract face area\n",
        "  face_box_orig=gray_img[y1:y2,x1:x2]\n",
        "  h_face, w_face = face_box_orig.shape[:2]\n",
        "  # resize this face image to 96x96\n",
        "  face_box_resized, vert_black, border_width = resize_image(face_box_orig, size=(SIZE,SIZE))\n",
        "  # if face_box_orig is not squared, a black border is added to make it squared before it is resized to SIZExSIZE\n",
        "  # there are two options: the image is higher than wide\n",
        "  # -> black border on the left and right side -> vert_black is 'True'\n",
        "  # -> black border on the bottom and the top -> vert_black is 'False'\n",
        "  # border_width: width of border on each side\n",
        "\n",
        "  # predict keypoints\n",
        "  keypoints = predict_keypoints(face_box_resized)\n",
        "  keypoints_reshaped = keypoints.reshape(-1,2)\n",
        "  # now the keypoints coordinates have to be transformed back into the size of \n",
        "  # the original webcam image:\n",
        "  # if black border had to be added to the left and right side \n",
        "  if vert_black:\n",
        "    x_coord = (keypoints_reshaped[:,0] * h_face)/SIZE  - border_width + x1\n",
        "    y_coord = (keypoints_reshaped[:,1] * h_face)/SIZE + y1\n",
        "  # or at the top and bottom:\n",
        "  else:\n",
        "    x_coord = (keypoints_reshaped[:,0] * w_face)/SIZE  + x1\n",
        "    y_coord = (keypoints_reshaped[:,1] * w_face)/SIZE - border_width + y1\n",
        "\n",
        "  x_coord = x_coord.round().astype(int)\n",
        "  y_coord = y_coord.round().astype(int) \n",
        "  \n",
        "  return x_coord, y_coord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkgvtpIV2zMz"
      },
      "source": [
        "We have one problem: The face recognized by the detector is not necessarily squared. If we take this part of the original image and resize it to 96x96 we receive a distorted image. That could deteriorate the accuracy of our keypoint detector. \n",
        "Idea: resize the face image while preserving the aspect ratio and fill the remaining pixels with black. As our model was trained with images that also contained black areas, that should not be a problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybP9flBy2yMM"
      },
      "source": [
        "# https://stackoverflow.com/questions/44650888/resize-an-image-without-distortion-opencv\n",
        "def resize_image(img, size=(SIZE,SIZE)):\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    c = img.shape[2] if len(img.shape)>2 else 1. \n",
        "    # because we have grayscale image as input -> c = 1\n",
        "\n",
        "    if h == w: \n",
        "        return cv2.resize(img, size, cv2.INTER_AREA)\n",
        "\n",
        "    vert_black = h > w\n",
        "    if vert_black:\n",
        "      dif = h\n",
        "    else:\n",
        "      dif = w\n",
        "\n",
        "    interpolation = cv2.INTER_AREA if dif > (size[0]+size[1])//2 else cv2.INTER_CUBIC\n",
        "\n",
        "    x_pos = (dif - w)//2\n",
        "    y_pos = (dif - h)//2\n",
        "\n",
        "    if vert_black:\n",
        "      border_width = x_pos\n",
        "    else:\n",
        "      border_width = y_pos\n",
        "\n",
        "    if len(img.shape) == 2:\n",
        "        mask = np.zeros((dif, dif), dtype=img.dtype)\n",
        "        mask[y_pos:y_pos+h, x_pos:x_pos+w] = img[:h, :w]\n",
        "    else:\n",
        "        mask = np.zeros((dif, dif, c), dtype=img.dtype)\n",
        "        mask[y_pos:y_pos+h, x_pos:x_pos+w, :] = img[:h, :w, :]\n",
        "\n",
        "    return cv2.resize(mask, size, interpolation), vert_black, border_width"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j75bU8-i4sxx"
      },
      "source": [
        "def predict_keypoints(img):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    orig_img = img.reshape(SIZE, SIZE).copy()\n",
        "    img = img.reshape(1, SIZE, SIZE)\n",
        "    img = img / 255.0\n",
        "    img = torch.tensor(img, dtype=torch.float)\n",
        "    img = img.unsqueeze(0).to(DEVICE)\n",
        "        \n",
        "    # forward pass through the model\n",
        "    keypoints = model(img).cpu().detach().numpy()\n",
        "  return keypoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz6msKLGycom",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "4fb6df83-d946-43e3-833a-c1543efece3c"
      },
      "source": [
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(output_path + filename))\n",
        "\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-102c15e324bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_photo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saved to {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Show the image which was just taken.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-1cfefa98a621>\u001b[0m in \u001b[0;36mtake_photo\u001b[0;34m(filename, quality)\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;31m###### start: new code #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;31m# get photo data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'takePhoto({})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquality\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m   \u001b[0;31m# get OpenCV format image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjs_to_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2Hsdwz_VbBH"
      },
      "source": [
        "# 3) Now, let's try a live video stream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzDPLWqZdsWP"
      },
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1qKldbBAe8n"
      },
      "source": [
        "Prediction without facial keypoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZaviKH0KAh88",
        "outputId": "844e32ce-9906-4cbf-887d-31d25e810600"
      },
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0, (300, 300), (104.0, 117.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    faces = net.forward()\n",
        "\n",
        "    # add face bounding boxes to image\n",
        "    for i in range(faces.shape[2]):\n",
        "      confidence = faces[0, 0, i, 2]\n",
        "      if confidence > 0.5:\n",
        "        box = faces[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "        (x, y, x1, y1) = box.astype(\"int\")\n",
        "        cv2.rectangle(bbox_array, (x,y), (x1, y1), (255, 0, 0), 2) \n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNR6HcnNAbyw"
      },
      "source": [
        "Prediction with facial keypoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf7rPvYo85PB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "fd0ca15e-144f-4457-9b90-5c4b9b00200b"
      },
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # grayscale image for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0, (300, 300), (104.0, 117.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    faces = net.forward()\n",
        "\n",
        "    # add face bounding boxes to image\n",
        "    for i in range(faces.shape[2]):\n",
        "      confidence = faces[0, 0, i, 2]\n",
        "      if confidence > 0.5:\n",
        "        box = faces[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "        (x1, y1, x2, y2) = box.astype(\"int\")\n",
        "        cv2.rectangle(bbox_array, (x1,y1), (x2, y2), (255, 0, 0), 2) \n",
        "\n",
        "        # keypoint prediction\n",
        "        x_coord, y_coord = make_prediction(gray, x1, y1, x2, y2)\n",
        "        # add keypoints to image\n",
        "        for i in range(len(x_coord)):\n",
        "            bbox_array = cv2.circle(bbox_array, (x_coord[i],y_coord[i]), radius=3, color=(0, 0, 255), thickness=-1)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpMS2ak8k40j"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}